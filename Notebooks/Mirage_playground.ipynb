{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2e0c9-41f0-46ac-bf77-80b4b3c6feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Any\n",
    "import subprocess\n",
    "import os\n",
    "from cv2.typing import MatLike\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from mirage.mirage_helpers import *\n",
    "from mirage.pose_extract_base import MLAbstractInterface\n",
    "from mirage.rgb_interface import CameraInterface\n",
    "from mirage.movenet import MovenetInterface\n",
    "from mirage.skeleton import SkeletonDetection\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showim(im):\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, int(np.ceil(n_images/float(cols))), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8078e3b-8b5a-4d97-a4f6-2f43ca50c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def ost_to_np(filename):\n",
    "    with open(filename) as s:\n",
    "        try:\n",
    "            q = yaml.safe_load(s)\n",
    "            return {\n",
    "                \"image_width\": q[\"image_width\"],\n",
    "                \"image_height\": q[\"image_height\"],\n",
    "                \"camera_name\": q[\"camera_name\"],\n",
    "                \"camera_matrix\": np.array(q[\"camera_matrix\"][\"data\"]).reshape(\n",
    "                    (q[\"camera_matrix\"][\"rows\"], q[\"camera_matrix\"][\"cols\"])\n",
    "                ),\n",
    "                \"distortion_coefficients\": np.array(q[\"distortion_coefficients\"][\"data\"]).reshape(\n",
    "                    (q[\"distortion_coefficients\"][\"rows\"], q[\"distortion_coefficients\"][\"cols\"])\n",
    "                ),\n",
    "            }\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return None\n",
    "\n",
    "\n",
    "def process_and_viz_split(i_cam: CameraInterface, ml_interface: MLAbstractInterface) -> MatLike:\n",
    "    img = i_cam.get_next_frame()\n",
    "    img_a, img_b = split_image_stack(img)\n",
    "    a_crop = determine_crop_region(s_a, img_a.shape[0], img_a.shape[1])\n",
    "    b_crop = determine_crop_region(s_b, img_b.shape[0], img_b.shape[1])\n",
    "    img_a_kp = ml_interface.predict(img_a, a_crop)\n",
    "    img_b_kp = ml_interface.predict(img_b, b_crop)\n",
    "    s_a.update_predictions(img_a_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    s_b.update_predictions(img_b_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    img_a_viz = skeleton_to_image(img_a, s_a)\n",
    "    img_b_viz = skeleton_to_image(img_b, s_b)\n",
    "    return stack_image(img_a_viz, img_b_viz)\n",
    "\n",
    "\n",
    "def process_and_viz(i_cam: CameraInterface, ml_interface: MLAbstractInterface) -> MatLike:\n",
    "    img = i_cam.get_next_frame()\n",
    "    img_kp = ml_interface.predict(img)\n",
    "    s_a.update_predictions(img_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    img_viz = skeleton_to_image(img, s_a)\n",
    "    return img_viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7996fa0-fc20-4b01-8878-9a4788245f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../../Data/Reference_Parayno_Jeru.mp4\"\n",
    "frame_number_start = 150\n",
    "frame_number_end = 160\n",
    "frames = []\n",
    "splitimage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0068289-58e5-404e-9f18-780c2f1ca68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "ml_interface = MovenetInterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4f2a4-dff3-494f-ac8c-c7569f72edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a = SkeletonDetection()\n",
    "s_b = SkeletonDetection()\n",
    "i_cam = CameraInterface(input_file)\n",
    "i_cam.set_frame(frame_number_start)\n",
    "frame_number_end = frame_number_end if frame_number_end != 0 else i_cam.get_total_frames()\n",
    "##### reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69600ef-ad17-4406-b61c-6dc6cc1ed468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_log_info(im: MatLike, skele: SkeletonDetection):\n",
    "    img = im.copy()\n",
    "    num_joints = len(skele.joints)\n",
    "    print(num_joints)\n",
    "    starting_y = 150\n",
    "    starting_x = im.shape[1]-350\n",
    "    height_per_text = 30\n",
    "    img = overlay_rect(img, starting_x - 30, 20, im.shape[1] - starting_x+20, im.shape[0] - 40)\n",
    "    for j in skele.joints.values():\n",
    "        out = f\"{j.name}: {j.confidence*100:.0f}%\"\n",
    "        img = cv2.putText(img, f\"{out:<20}\", (starting_x, starting_y), cv2.FONT_HERSHEY_PLAIN, 2, (255,0,255), 2)\n",
    "        starting_y += height_per_text\n",
    "    return img\n",
    "\n",
    "def overlay_rect(img: MatLike, x: int, y: int, w: int, h: int):\n",
    "    # First we crop the sub-rect from the image\n",
    "    sub_img = img[y:y+h, x:x+w]\n",
    "    white_rect = np.ones(sub_img.shape, dtype=np.uint8) * 0\n",
    "    res = cv2.addWeighted(sub_img, 0.25, white_rect, 0.75, 1.0)\n",
    "    # Putting the image back to its position\n",
    "    img[y:y+h, x:x+w] = res\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6416e66-d775-4e72-ac9e-d0330a9febfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = i_cam.get_next_frame()\n",
    "img_a, img_b = split_image_stack(img)\n",
    "a_crop = determine_crop_region(s_a, img_a.shape[0], img_a.shape[1])\n",
    "b_crop = determine_crop_region(s_b, img_b.shape[0], img_b.shape[1])\n",
    "\n",
    "print(a_crop)\n",
    "if a_crop is not None:\n",
    "    a_crop_img = crop_image(img_a, int(a_crop[\"y_min\"]),\n",
    "                    int(a_crop[\"height\"]),\n",
    "                    int(a_crop[\"x_min\"]),\n",
    "                    int(a_crop[\"width\"]),\n",
    "                    0,)\n",
    "else:\n",
    "    a_crop_img = img_a * 0\n",
    "img_a_kp = ml_interface.predict(img_a, a_crop)\n",
    "s_a.update_predictions(img_a_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "img_a_viz = skeleton_to_image(img_a, s_a)\n",
    "\n",
    "show_images([img_a, a_crop_img, img_a_viz], 2, [\"original image\", \"crop used\", \"Estimated points\"])\n",
    "# showim(display_log_info(img_a, s_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c54f24-dc39-42cc-9972-c3dbddb4d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [tuple(np.random.randint(0,255,3).tolist()) for j in range(100)]\n",
    "def drawlines(img1,img2,lines,pts1,pts2):\n",
    " ''' img1 - image on which we draw the epilines for the points in img2\n",
    " lines - corresponding epilines '''\n",
    " r,c = img1.shape\n",
    " img1 = cv2.cvtColor(img1,cv2.COLOR_GRAY2BGR)\n",
    " img2 = cv2.cvtColor(img2,cv2.COLOR_GRAY2BGR)\n",
    " for i, (r,pt1,pt2) in enumerate(zip(lines,pts1,pts2)):\n",
    "     color = color_list[i%len(color_list)]\n",
    "     x0,y0 = map(int, [0, -r[2]/r[1] ])\n",
    "     x1,y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "     img1 = cv2.line(img1, (x0,y0), (x1,y1), color,1)\n",
    "     img1 = cv2.circle(img1,tuple(np.int32(pt1)),5,color,-1)\n",
    "     img2 = cv2.circle(img2,tuple(np.int32(pt2)),5,color,-1)\n",
    " return img1,img2\n",
    "\n",
    "def drawpoints(img1, pts1):\n",
    "    r,c = img1.shape\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "    for i, pt in enumerate(pts1):\n",
    "        color = color_list[i%len(color_list)]\n",
    "        im1 = cv2.circle(img1, tuple(np.int32(pt)), 9, color, 3)\n",
    "    return img1\n",
    "\n",
    "    \n",
    "# pts1 = np.float64(pts1)\n",
    "# pts2 = np.float64(pts2)\n",
    "# E, mask = cv2.findEssentialMat(pts1, pts2, cam_a_intr[\"camera_matrix\"], cam_a_intr[\"distortion_coefficients\"], cam_b_intr[\"camera_matrix\"], cam_b_intr[\"distortion_coefficients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66c25e-8298-49f6-b016-abe300b20928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = i_cam.get_next_frame()\n",
    "img_a, img_b = split_image_stack(img)\n",
    "img_a = cv2.cvtColor( img_a, cv2.COLOR_BGR2GRAY)\n",
    "img_b = cv2.cvtColor( img_b, cv2.COLOR_BGR2GRAY)\n",
    "cam_b_intr = ost_to_np(\"cama_ost.yaml\")\n",
    "cam_a_intr = ost_to_np(\"camb_ost.yaml\")\n",
    "\n",
    "img_a_undistorted = cv2.undistort(img_a, cam_a_intr[\"camera_matrix\"], cam_a_intr[\"distortion_coefficients\"], None, None)\n",
    "img_b_undistorted = cv2.undistort(img_b, cam_b_intr[\"camera_matrix\"], cam_b_intr[\"distortion_coefficients\"], None, None)\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "kp1, des1 = sift.detectAndCompute(img_a_undistorted, None)\n",
    "kp2, des2 = sift.detectAndCompute(img_b_undistorted, None)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for i, (m,n) in enumerate(matches):\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "\n",
    "\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "F, mask = cv2.findFundamentalMat(pts1, pts2, cv2.FM_8POINT)\n",
    "pts1 = pts1[mask.ravel()==1]\n",
    "pts2 = pts2[mask.ravel()==1]\n",
    "\n",
    "lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)\n",
    "lines1 = lines1.reshape(-1, 3)\n",
    "img_a_pts = drawpoints(img_a, pts1)\n",
    "img_a_epi, _ = drawlines(img_a, img_b, lines1, pts1, pts2)\n",
    "\n",
    "lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)\n",
    "lines2 = lines2.reshape(-1, 3)\n",
    "img_b_pts = drawpoints(img_b, pts2)\n",
    "img_b_epi, _ = drawlines(img_b, img_a, lines2, pts2, pts1)\n",
    "\n",
    "\n",
    "print(F)\n",
    "show_images([img_a, img_b, img_a_undistorted, img_b_undistorted, img_a_pts, img_b_pts, img_a_epi, img_b_epi],\n",
    "            4, \n",
    "            [\"original image a\", \"original image b\", \"undist A\", \"undist B\", \"points a\", \"points b\", \"epi a\", \"epi b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881c8bf-d1e5-422c-a921-ade7dfa68f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cam_a_intr[\"camera_matrix\"])\n",
    "print(cam_b_intr[\"camera_matrix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18b065-3090-43c9-ab52-c1ec342ded35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b769e-ba0e-4f18-b3b1-b34898b1a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{m} :::: {n}\" for (m, n) in zip(pts1, pts2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33953893-9fe8-46dc-8c12-e3891cb659ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MirageEnv",
   "language": "python",
   "name": "mirageenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
