{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2e0c9-41f0-46ac-bf77-80b4b3c6feca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Any\n",
    "import subprocess\n",
    "import os\n",
    "from cv2.typing import MatLike\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "from mirage.mirage_helpers import *\n",
    "from mirage.pose_extract_base import MLAbstractInterface\n",
    "from mirage.rgb_interface import CameraInterface\n",
    "from mirage.movenet import MovenetInterface\n",
    "from mirage.skeleton import SkeletonDetection\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showim(im):\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "\n",
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images in a single figure with matplotlib.\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols (Default = 1): Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, int(np.ceil(n_images/float(cols))), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8078e3b-8b5a-4d97-a4f6-2f43ca50c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_and_viz_split(i_cam: CameraInterface, ml_interface: MLAbstractInterface) -> MatLike:\n",
    "    img = i_cam.get_next_frame()\n",
    "    img_a, img_b = split_image_stack(img)\n",
    "    a_crop = determine_crop_region(s_a, img_a.shape[0], img_a.shape[1])\n",
    "    b_crop = determine_crop_region(s_b, img_b.shape[0], img_b.shape[1])\n",
    "    img_a_kp = ml_interface.predict(img_a, a_crop)\n",
    "    img_b_kp = ml_interface.predict(img_b, b_crop)\n",
    "    s_a.update_predictions(img_a_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    s_b.update_predictions(img_b_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    img_a_viz = skeleton_to_image(img_a, s_a)\n",
    "    img_b_viz = skeleton_to_image(img_b, s_b)\n",
    "    return stack_image(img_a_viz, img_b_viz)\n",
    "\n",
    "\n",
    "def process_and_viz(i_cam: CameraInterface, ml_interface: MLAbstractInterface) -> MatLike:\n",
    "    img = i_cam.get_next_frame()\n",
    "    img_kp = ml_interface.predict(img)\n",
    "    s_a.update_predictions(img_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "    img_viz = skeleton_to_image(img, s_a)\n",
    "    return img_viz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7996fa0-fc20-4b01-8878-9a4788245f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"../../Data/Reference_Parayno_Jeru.mp4\"\n",
    "frame_number_start = 150\n",
    "frame_number_end = 160\n",
    "frames = []\n",
    "splitimage = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0068289-58e5-404e-9f18-780c2f1ca68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "ml_interface = MovenetInterface()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bfb49-cac8-4794-8f03-bae2153edb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_crop_region(skele: SkeletonDetection, image_height, image_width) -> dict[str, float]:\n",
    "    \"\"\"determine the crop region to run inference, uses the skeleton detection to get\n",
    "    a square region that encloses the full body of the target person.  when not confident in\n",
    "    torso projections, falls back on full image padded to square. Modified from movenet tutorial\n",
    "    \"\"\"\n",
    "    if not torso_visible(skele):\n",
    "        return default_crop_region(image_height, image_width)\n",
    "    center_y = (skele.joints[11].current_xy[1] + skele.joints[12].current_xy[1]) / 2\n",
    "    center_x = (skele.joints[11].current_xy[0] + skele.joints[12].current_xy[0]) / 2\n",
    "    (max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange) = determine_torso_and_body_range(\n",
    "        skele, center_y, center_x, image_width, image_height\n",
    "    )\n",
    "    # from ratio to resolution space\n",
    "    max_body_yrange *= image_height\n",
    "    max_body_xrange *= image_width\n",
    "    max_torso_yrange *= image_height\n",
    "    max_torso_xrange *= image_width\n",
    "    center_x *= image_width\n",
    "    center_y *= image_height\n",
    "\n",
    "    ranges = [max_torso_xrange * 2, max_torso_yrange * 2, max_body_xrange * 2, max_body_yrange * 2]\n",
    "    crop_length_half = np.amax(ranges)\n",
    "    tmp = np.array([center_x, image_width - center_x, center_y, image_height - center_y])\n",
    "    crop_length_half = np.amin([crop_length_half, np.amax(tmp)])\n",
    "    crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
    "\n",
    "    if crop_length_half > max(image_width, image_height) / 2:\n",
    "        return default_crop_region(image_height, image_width)\n",
    "    else:\n",
    "        crop_length = crop_length_half * 2\n",
    "        return {\n",
    "            \"y_min\": crop_corner[0],\n",
    "            \"x_min\": crop_corner[1],\n",
    "            \"height\": (crop_corner[0] + crop_length) - crop_corner[0],\n",
    "            \"width\": (crop_corner[1] + crop_length) - crop_corner[1],\n",
    "            \"clh\": crop_length_half,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10da50-9e93-4116-90d9-36b7d5c3e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_coord(image: MatLike, keypoint: tuple[int, int, int]) -> tuple[int, int]:\n",
    "    ratio = image.shape[1] / image.shape[0]\n",
    "    # assuming landscape.. and padded to be square. TODO: make more robust or informed.\n",
    "    padding_val = (image.shape[0] - image.shape[1]) / 2\n",
    "    y_val: float = keypoint[0]\n",
    "    x_val: float = keypoint[1]\n",
    "    return (int(x_val * image.shape[1]), int(y_val * image.shape[0] * ratio + padding_val))\n",
    "\n",
    "\n",
    "def skeleton_to_image(image: MatLike, skele: SkeletonDetection, min_confidence: float = 0.2):\n",
    "    drawn_image: MatLike = image.copy()\n",
    "    for i, joint in skele.joints.items():\n",
    "        kpmap = joint\n",
    "        if kpmap.display:\n",
    "            y_val: float = joint.estimate[2]\n",
    "            x_val: float = joint.estimate[0]\n",
    "            confidence: float = joint.confidence\n",
    "            if confidence > min_confidence:\n",
    "                drawn_image = cv2.circle(\n",
    "                    drawn_image,\n",
    "                    k_coord(drawn_image, (y_val, x_val, 1)),\n",
    "                    radius=5,\n",
    "                    color=kpmap.color,\n",
    "                    thickness=2,\n",
    "                )\n",
    "    for edge_k, edge_v in KeypointEdges.items():\n",
    "        if skele.joints[edge_k[0]].display and skele.joints[edge_k[1]].display:\n",
    "            drawn_image = cv2.line(\n",
    "                drawn_image,\n",
    "                k_coord(drawn_image, (skele.joints[edge_k[0]].estimate[2], skele.joints[edge_k[0]].estimate[0])),\n",
    "                k_coord(drawn_image, (skele.joints[edge_k[1]].estimate[2], skele.joints[edge_k[1]].estimate[0])),\n",
    "                edge_v,\n",
    "                5,\n",
    "            )\n",
    "    return drawn_image\n",
    "\n",
    "def keypoint_to_original_image_space(keypoints: np.ndarray, image: MatLike, crop_region: dict[str, int] | None = None\n",
    ") -> np.ndarray:\n",
    "    if crop_region is None: \n",
    "        return keypoints\n",
    "    original_height, original_width, channels = image.shape\n",
    "    padd = (original_width - original_height)\n",
    "    for i in range(len(keypoints)):\n",
    "        keypoints[i][1] = (crop_region[\"x_min\"] + (keypoints[i][1] * float(crop_region[\"width\"]))) / original_width\n",
    "        keypoints[i][0] = (crop_region[\"y_min\"] + (padd/2) + (keypoints[i][0] * float(crop_region[\"height\"]))) / (original_height + padd)\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4f2a4-dff3-494f-ac8c-c7569f72edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_a = SkeletonDetection()\n",
    "s_b = SkeletonDetection()\n",
    "i_cam = CameraInterface(input_file)\n",
    "i_cam.set_frame(frame_number_start)\n",
    "frame_number_end = frame_number_end if frame_number_end != 0 else i_cam.get_total_frames()\n",
    "##### reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6416e66-d775-4e72-ac9e-d0330a9febfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = i_cam.get_next_frame()\n",
    "img_a, img_b = split_image_stack(img)\n",
    "a_crop = determine_crop_region(s_a, img_a.shape[0], img_a.shape[1])\n",
    "b_crop = determine_crop_region(s_b, img_b.shape[0], img_b.shape[1])\n",
    "\n",
    "print(a_crop)\n",
    "if a_crop is not None:\n",
    "    a_crop_img = crop_image(img_a, int(a_crop[\"y_min\"]),\n",
    "                    int(a_crop[\"height\"]),\n",
    "                    int(a_crop[\"x_min\"]),\n",
    "                    int(a_crop[\"width\"]),\n",
    "                    0,)\n",
    "else:\n",
    "    a_crop_img = img_a * 0\n",
    "img_a_kp_raw = ml_interface.predict(img_a, a_crop)\n",
    "img_a_kp = keypoint_to_original_image_space(img_a_kp_raw, img_a, a_crop)\n",
    "s_a.update_predictions(img_a_kp, (1.0 / i_cam.get_frame_rate_per_second()))\n",
    "img_a_viz = skeleton_to_image(img_a, s_a)\n",
    "\n",
    "show_images([img_a, a_crop_img, img_a_viz], 2, [\"original image\", \"crop used\", \"Estimated points\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66c25e-8298-49f6-b016-abe300b20928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881c8bf-d1e5-422c-a921-ade7dfa68f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MirageEnv",
   "language": "python",
   "name": "mirageenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
